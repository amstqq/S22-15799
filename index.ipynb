{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sql_metadata import Parser\n",
    "from collections import defaultdict\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql-2022-02-10_211004.csv postgresql-2022-02-10_211004.log\n"
     ]
    }
   ],
   "source": [
    "!ls /usr/local/var/postgres/log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_type</th>\n",
       "      <th>query_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>COMMIT</td>\n",
       "      <td>execute S_1: COMMIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>BEGIN</td>\n",
       "      <td>execute &lt;unnamed&gt;: BEGIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>UPDATE</td>\n",
       "      <td>execute &lt;unnamed&gt;: UPDATE review SET rating = $1 WHERE i_id=$2 AND u_id=$3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>COMMIT</td>\n",
       "      <td>execute S_1: COMMIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>BEGIN</td>\n",
       "      <td>execute &lt;unnamed&gt;: BEGIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>UPDATE</td>\n",
       "      <td>execute &lt;unnamed&gt;: UPDATE trust SET trust = $1 WHERE source_u_id=$2 AND target_u_id=$3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>COMMIT</td>\n",
       "      <td>execute S_1: COMMIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>BEGIN</td>\n",
       "      <td>execute &lt;unnamed&gt;: BEGIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>UPDATE</td>\n",
       "      <td>execute &lt;unnamed&gt;: UPDATE item SET title = $1 WHERE i_id=$2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>COMMIT</td>\n",
       "      <td>execute S_1: COMMIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_type  \\\n",
       "70     COMMIT   \n",
       "71      BEGIN   \n",
       "72     UPDATE   \n",
       "73     COMMIT   \n",
       "74      BEGIN   \n",
       "75     UPDATE   \n",
       "76     COMMIT   \n",
       "77      BEGIN   \n",
       "78     UPDATE   \n",
       "79     COMMIT   \n",
       "\n",
       "                                                                                query_text  \n",
       "70                                                                     execute S_1: COMMIT  \n",
       "71                                                                execute <unnamed>: BEGIN  \n",
       "72              execute <unnamed>: UPDATE review SET rating = $1 WHERE i_id=$2 AND u_id=$3  \n",
       "73                                                                     execute S_1: COMMIT  \n",
       "74                                                                execute <unnamed>: BEGIN  \n",
       "75  execute <unnamed>: UPDATE trust SET trust = $1 WHERE source_u_id=$2 AND target_u_id=$3  \n",
       "76                                                                     execute S_1: COMMIT  \n",
       "77                                                                execute <unnamed>: BEGIN  \n",
       "78                             execute <unnamed>: UPDATE item SET title = $1 WHERE i_id=$2  \n",
       "79                                                                     execute S_1: COMMIT  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG_DIRECTORY = 'epinions_workload.csv'\n",
    "df = pd.read_csv(LOG_DIRECTORY, header=None, usecols=[7, 13], names=[\"query_type\", \"query_text\"])\n",
    "df.iloc[70:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_to_columns_epinions = {\n",
    "  \"item\": set([\"i_id\", \"title\"]),\n",
    "  \"useracct\": set([\"u_id\", \"name\"]),\n",
    "  \"review\": set([\"a_id\", \"u_id\", \"i_id\", \"rating\", \"rank\"]),\n",
    "  \"trust\": set([\"source_u_id\", \"target_u_id\", \"trust\", \"creation_date\"]),\n",
    "  \"review_rating\": set([\"u_id\", \"a_id\", \"rating\", \"status\", \"creation_date\", \"last_mod_date\", \"type\", \"vertical_id\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = set()\n",
    "def parse_query(query_str : str, ttc_mapping : dict):\n",
    "  \n",
    "  if \"pg_\" in query_str:\n",
    "    return None\n",
    "\n",
    "  query_str = query_str[query_str.find(':')+1:] # Remove \"execute <unamed>:\"\n",
    "\n",
    "  # Ignore non-relevant queries\n",
    "  if \"BEGIN\" in query_str or \"COMMIT\" in query_str:\n",
    "    return None\n",
    "\n",
    "  try:\n",
    "    p = Parser(query_str)\n",
    "    tables = p.tables\n",
    "    columns = p.columns\n",
    "  except:\n",
    "    return None\n",
    "\n",
    "  # Skip queries that dont have a where clause\n",
    "  if len(tables) == 0 or len(columns) == 0:\n",
    "    return None\n",
    "\n",
    "  # Build string \"table.column_name\" for each column\n",
    "  added = False\n",
    "  for column in columns:\n",
    "    if \".\" in column:\n",
    "      added = True\n",
    "    else:\n",
    "      # Find which table this column corresponds to\n",
    "      for table in tables:\n",
    "        if column in ttc_mapping[table]:\n",
    "          added = True\n",
    "          break\n",
    "  \n",
    "  if not added:\n",
    "    # print(\"Invalid Query:\", query_str)\n",
    "    return None\n",
    "  return query_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not supported query type:  SET extra_float_digits = 3\n",
      "Not supported query type:  SET application_name = 'PostgreSQL JDBC Driver'\n",
      "Not supported query type:  SET extra_float_digits = 3\n",
      "Not supported query type:  SET application_name = 'PostgreSQL JDBC Driver'\n",
      "Not supported query type:  SET extra_float_digits = 3\n",
      "Not supported query type:  SET application_name = 'PostgreSQL JDBC Driver'\n",
      "Not supported query type:  SET SESSION CHARACTERISTICS AS TRANSACTION ISOLATION LEVEL SERIALIZABLE\n",
      "Not supported query type:  SET extra_float_digits = 3\n",
      "Not supported query type:  SET application_name = 'PostgreSQL JDBC Driver'\n",
      "Not supported query type:  SHOW ALL\n",
      "Not supported query type:  ALTER SYSTEM SET log_destination='stderr'\n",
      "Not supported query type:  ALTER SYSTEM SET logging_collector='off'\n",
      "Not supported query type:  ALTER SYSTEM SET log_statement='none'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{' SELECT * FROM review r WHERE r.i_id=$1 ORDER BY creation_date DESC',\n",
       " ' SELECT * FROM review r, item i WHERE i.i_id = r.i_id and r.i_id=$1 ORDER BY rating DESC, r.creation_date DESC LIMIT 10',\n",
       " ' SELECT * FROM review r, useracct u WHERE u.u_id = r.u_id AND r.u_id=$1 ORDER BY rating DESC, r.creation_date DESC LIMIT 10',\n",
       " ' SELECT * FROM trust t WHERE t.source_u_id=$1',\n",
       " ' SELECT avg(rating) FROM review r WHERE r.i_id=$1',\n",
       " ' SELECT avg(rating) FROM review r, trust t WHERE r.u_id=t.target_u_id AND r.i_id=$1 AND t.source_u_id=$2',\n",
       " ' SELECT i_id FROM item',\n",
       " ' SELECT u_id FROM useracct',\n",
       " ' UPDATE item SET title = $1 WHERE i_id=$2',\n",
       " ' UPDATE review SET rating = $1 WHERE i_id=$2 AND u_id=$3',\n",
       " ' UPDATE trust SET trust = $1 WHERE source_u_id=$2 AND target_u_id=$3',\n",
       " ' UPDATE useracct SET name = $1 WHERE u_id=$2'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = []\n",
    "for index, row in df.iterrows():\n",
    "  if pd.isna(row[\"query_type\"]):\n",
    "    continue\n",
    "  q = parse_query(row[\"query_text\"], table_to_columns_epinions)\n",
    "  if q is not None:\n",
    "    queries.append(q)\n",
    "\n",
    "set(queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_actions_str(indexes : list, filename : str) -> list:\n",
    "  cmds = [f'echo \"Generating sql commands for {filename}\"']\n",
    "  for i, index in enumerate(indexes):\n",
    "    table, column = index.split(\".\")\n",
    "    if i == 0:\n",
    "      cmd = f'echo \"CREATE INDEX idx_{table}_{column} ON {table}({column});\" > {filename}'\n",
    "    else:\n",
    "      cmd = f'echo \"CREATE INDEX idx_{table}_{column} ON {table}({column});\" >> {filename}'\n",
    "    cmds.append(cmd)\n",
    "\n",
    "  return cmds\n",
    "\n",
    "\n",
    "def build_drop_idx_cmd(indexes : list) -> str:\n",
    "  drop_cmd = \"DROP INDEX\"\n",
    "  for i, index in enumerate(indexes):\n",
    "    table, column = index.split(\".\")\n",
    "    if i != len(indexes)-1:\n",
    "      drop_cmd += f\" idx_{table}_{column},\"\n",
    "    else:\n",
    "      drop_cmd += f\" idx_{table}_{column};\"\n",
    "\n",
    "  return drop_cmd\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import pglast\n",
    "from pandarallel import pandarallel\n",
    "from plumbum import cli\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "# Enable parallel pandas operations.\n",
    "# pandarallel is a little buggy. For example, progress_bar=True does not work,\n",
    "# and if you are using PyCharm you will want to enable \"Emulate terminal in\n",
    "# output console\" instead of using the PyCharm Python Console.\n",
    "# The reason we're using this library anyway is that:\n",
    "# - The parallelization is dead simple: change .blah() to .parallel_blah().\n",
    "# - swifter has poor string perf; we're mainly performing string ops.\n",
    "# - That said, Wan welcomes any switch that works.\n",
    "pandarallel.initialize(verbose=1)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    _PG_LOG_COLUMNS: List[str] = [\n",
    "        \"log_time\",\n",
    "        \"user_name\",\n",
    "        \"database_name\",\n",
    "        \"process_id\",\n",
    "        \"connection_from\",\n",
    "        \"session_id\",\n",
    "        \"session_line_num\",\n",
    "        \"command_tag\",\n",
    "        \"session_start_time\",\n",
    "        \"virtual_transaction_id\",\n",
    "        \"transaction_id\",\n",
    "        \"error_severity\",\n",
    "        \"sql_state_code\",\n",
    "        \"message\",\n",
    "        \"detail\",\n",
    "        \"hint\",\n",
    "        \"internal_query\",\n",
    "        \"internal_query_pos\",\n",
    "        \"context\",\n",
    "        \"query\",\n",
    "        \"query_pos\",\n",
    "        \"location\",\n",
    "        \"application_name\",\n",
    "        \"backend_type\",\n",
    "    ]\n",
    "\n",
    "    \"\"\"\n",
    "    Convert PostgreSQL query logs into pandas DataFrame objects.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        \"\"\"\n",
    "        Get a raw dataframe of query log data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df : pd.DataFrame\n",
    "            Dataframe containing the query log data.\n",
    "            Note that irrelevant query log entries are still included.\n",
    "        \"\"\"\n",
    "        return self._df\n",
    "\n",
    "    def get_grouped_dataframe_interval(self, interval=None):\n",
    "        \"\"\"\n",
    "        Get the pre-grouped version of query log data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        interval : pd.TimeDelta or None\n",
    "            time interval to group and count the query templates\n",
    "            if None, pd is only aggregated by template\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        grouped_df : pd.DataFrame\n",
    "            Dataframe containing the pre-grouped query log data.\n",
    "            Grouped on query template and optionally log time.\n",
    "        \"\"\"\n",
    "        gb = None\n",
    "        if interval is None:\n",
    "            gb = self._df.groupby(\"query_template\").size()\n",
    "            gb.drop(\"\", axis=0, inplace=True)\n",
    "        else:\n",
    "            gb = self._df.groupby(\"query_template\").resample(interval).size()\n",
    "            gb.drop(\"\", axis=0, level=0, inplace=True)\n",
    "        grouped_df = pd.DataFrame(gb, columns=[\"count\"])\n",
    "        return grouped_df\n",
    "\n",
    "    def get_grouped_dataframe_params(self):\n",
    "        \"\"\"\n",
    "        Get the pre-grouped version of query log data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        grouped_df : pd.DataFrame\n",
    "            Dataframe containing the pre-grouped query log data.\n",
    "            Grouped on query template and query parameters.\n",
    "        \"\"\"\n",
    "        return self._grouped_df_params\n",
    "\n",
    "    def get_params(self, query):\n",
    "        \"\"\"\n",
    "        Find the parameters associated with a particular query.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query : str\n",
    "            The query template to look up parameters for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        params : pd.Series\n",
    "            The counts of parameters associated with a particular query.\n",
    "            Unfortunately, due to quirks of the PostgreSQL CSVLOG format,\n",
    "            the types of parameters are unreliable and may be stringly typed.\n",
    "        \"\"\"\n",
    "        params = self._grouped_df_params.query(\"query_template == @query\")\n",
    "        return params.droplevel(0).squeeze(axis=1)\n",
    "\n",
    "    def sample_params(self, query, n, replace=True, weights=True):\n",
    "        \"\"\"\n",
    "        Find a sampling of parameters associated with a particular query.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query : str\n",
    "            The query template to look up parameters for.\n",
    "        n : int\n",
    "            The number of parameter vectors to sample.\n",
    "        replace : bool\n",
    "            True if the sampling should be done with replacement.\n",
    "        weights : bool\n",
    "            True if the sampling should use the counts as weights.\n",
    "            False if the sampling should be equal probability weighting.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        params : np.ndarray\n",
    "            Sample of the parameters associated with a particular query.\n",
    "        \"\"\"\n",
    "        params = self.get_params(query)\n",
    "        weight_vec = params if weights else None\n",
    "        sample = params.sample(n, replace=replace, weights=weight_vec)\n",
    "        return sample.index.to_numpy()\n",
    "\n",
    "    @staticmethod\n",
    "    def substitute_params(query_template, params):\n",
    "        assert type(query_template) == str\n",
    "        query = query_template\n",
    "        keys = [f\"${i}\" for i in range(1, len(params) + 1)]\n",
    "        for k, v in reversed(list(zip(keys, params))):\n",
    "            # The reversing is crucial! Note that $1 is a prefix of $10.\n",
    "            query = query.replace(k, v)\n",
    "        return query\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_csv(csvlog, log_columns):\n",
    "        \"\"\"\n",
    "        Read a PostgreSQL CSVLOG file into a pandas DataFrame.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        csvlog : str\n",
    "            Path to a CSVLOG file generated by PostgreSQL.\n",
    "        log_columns : List[str]\n",
    "            List of columns in the csv log.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df : pd.DataFrame\n",
    "            DataFrame containing the relevant columns for query forecasting.\n",
    "        \"\"\"\n",
    "        # This function must have a separate non-local binding from _read_df\n",
    "        # so that it can be pickled for multiprocessing purposes.\n",
    "        return pd.read_csv(\n",
    "            csvlog,\n",
    "            names=log_columns,\n",
    "            parse_dates=[\"log_time\", \"session_start_time\"],\n",
    "            usecols=[\n",
    "                \"log_time\",\n",
    "                \"session_start_time\",\n",
    "                \"command_tag\",\n",
    "                \"message\",\n",
    "                \"detail\",\n",
    "            ],\n",
    "            header=None,\n",
    "            index_col=False,\n",
    "            skiprows=100,\n",
    "            nrows=1000\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_query(message_series):\n",
    "        \"\"\"\n",
    "        Extract SQL queries from the CSVLOG's message column.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        message_series : pd.Series\n",
    "            A series corresponding to the message column of a CSVLOG file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        query : pd.Series\n",
    "            A str-typed series containing the queries from the log.\n",
    "        \"\"\"\n",
    "        simple = r\"statement: ((?:DELETE|INSERT|SELECT|UPDATE).*)\"\n",
    "        extended = r\"execute .+: ((?:DELETE|INSERT|SELECT|UPDATE).*)\"\n",
    "        regex = f\"(?:{simple})|(?:{extended})\"\n",
    "        query = message_series.str.extract(regex, flags=re.IGNORECASE)\n",
    "        # Combine the capture groups for simple and extended query protocol.\n",
    "        query = query[0].fillna(query[1])\n",
    "        print(\"TODO(WAN): Disabled SQL format for being too slow.\")\n",
    "        # Prettify each SQL query for standardized formatting.\n",
    "        # query = query.parallel_map(pglast.prettify, na_action='ignore')\n",
    "        # Replace NA values (irrelevant log messages) with empty strings.\n",
    "        query.fillna(\"\", inplace=True)\n",
    "        return query.astype(str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_params(detail_series):\n",
    "        \"\"\"\n",
    "        Extract SQL parameters from the CSVLOG's detail column.\n",
    "        If there are no such parameters, an empty {} is returned.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        detail_series : pd.Series\n",
    "            A series corresponding to the detail column of a CSVLOG file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        params : pd.Series\n",
    "            A dict-typed series containing the parameters from the log.\n",
    "        \"\"\"\n",
    "\n",
    "        def extract(detail):\n",
    "            detail = str(detail)\n",
    "            prefix = \"parameters: \"\n",
    "            idx = detail.find(prefix)\n",
    "            if idx == -1:\n",
    "                return {}\n",
    "            parameter_list = detail[idx + len(prefix):]\n",
    "            params = {}\n",
    "            for pstr in parameter_list.split(\", \"):\n",
    "                pnum, pval = pstr.split(\" = \")\n",
    "                assert pnum.startswith(\"$\")\n",
    "                assert pnum[1:].isdigit()\n",
    "                params[pnum] = pval\n",
    "            return params\n",
    "\n",
    "        return detail_series.parallel_apply(extract)\n",
    "\n",
    "    @staticmethod\n",
    "    def _substitute_params(df, query_col, params_col):\n",
    "        \"\"\"\n",
    "        Substitute parameters into the query, wherever possible.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            The dataframe of query log data.\n",
    "        query_col : str\n",
    "            Name of the query column produced by _extract_query.\n",
    "        params_col : str\n",
    "            Name of the parameter column produced by _extract_params.\n",
    "        Returns\n",
    "        -------\n",
    "        query_subst : pd.Series\n",
    "            A str-typed series containing the query with parameters inlined.\n",
    "        \"\"\"\n",
    "\n",
    "        def substitute(query, params):\n",
    "            # Consider '$2' -> \"abc'def'ghi\".\n",
    "            # This necessitates the use of a SQL-aware substitution,\n",
    "            # even if this is much slower than naive string substitution.\n",
    "            new_sql, last_end = [], 0\n",
    "            for token in pglast.parser.scan(query):\n",
    "                token_str = str(query[token.start: token.end + 1])\n",
    "                if token.start > last_end:\n",
    "                    new_sql.append(\" \")\n",
    "                if token.name == \"PARAM\":\n",
    "                    assert token_str.startswith(\"$\")\n",
    "                    assert token_str[1:].isdigit()\n",
    "                    new_sql.append(params[token_str])\n",
    "                else:\n",
    "                    new_sql.append(token_str)\n",
    "                last_end = token.end + 1\n",
    "            new_sql = \"\".join(new_sql)\n",
    "            return new_sql\n",
    "\n",
    "        def subst(row):\n",
    "            return substitute(row[query_col], row[params_col])\n",
    "\n",
    "        return df.parallel_apply(subst, axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse(query_series):\n",
    "        \"\"\"\n",
    "        Parse the SQL query to extract (prepared queries, parameters).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query_series : pd.Series\n",
    "            SQL queries with the parameters inlined.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        queries_and_params : pd.Series\n",
    "            A series containing tuples of (prepared SQL query, parameters).\n",
    "        \"\"\"\n",
    "        simple = r\"statement: ((?:DELETE|INSERT|SELECT|UPDATE).*)\"\n",
    "        extended = r\"execute .+: ((?:DELETE|INSERT|SELECT|UPDATE).*)\"\n",
    "        regex = f\"(?:{simple})|(?:{extended})\"\n",
    "        def parse(sql):\n",
    "            query = re.search(regex, sql)\n",
    "            if not query:\n",
    "                return np.nan, ()\n",
    "            query = query.group(0)\n",
    "\n",
    "            new_sql, params, last_end = [], [], 0\n",
    "            for token in pglast.parser.scan(sql):\n",
    "                token_str = str(sql[token.start: token.end + 1])\n",
    "                if token.start > last_end:\n",
    "                    new_sql.append(\" \")\n",
    "                if token.name in [\"ICONST\", \"FCONST\", \"SCONST\"]:\n",
    "                    # Integer, float, or string constant.\n",
    "                    new_sql.append(\"$\" + str(len(params) + 1))\n",
    "                    params.append(token_str)\n",
    "                else:\n",
    "                    new_sql.append(token_str)\n",
    "                last_end = token.end + 1\n",
    "            new_sql = \"\".join(new_sql)\n",
    "            return new_sql, tuple(params)\n",
    "\n",
    "        return query_series.parallel_apply(parse)\n",
    "\n",
    "    def _from_csvlogs(self, workload_csv_path, log_columns, store_query_subst=False):\n",
    "        \"\"\"\n",
    "        Glue code for initializing the Preprocessor from CSVLOGs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        csvlogs : List[str]\n",
    "            List of PostgreSQL CSVLOG files.\n",
    "        log_columns : List[str]\n",
    "            List of columns in the csv log.\n",
    "        store_query_subst: bool\n",
    "            True if the \"query_subst\" column should be stored.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df : pd.DataFrame\n",
    "            A dataframe representing the query log.\n",
    "        \"\"\"\n",
    "        time_end, time_start = None, time.perf_counter()\n",
    "\n",
    "        def clock(label):\n",
    "            nonlocal time_end, time_start\n",
    "            time_end = time.perf_counter()\n",
    "            print(\"\\r{}: {:.2f} s\".format(label, time_end - time_start))\n",
    "            time_start = time_end\n",
    "\n",
    "        df = self._read_csv(workload_csv_path, log_columns)\n",
    "        clock(\"Read dataframe\")\n",
    "\n",
    "        # print(\"Extract queries: \", end=\"\", flush=True)\n",
    "        # df[\"query_raw\"] = self._extract_query(df[\"message\"])\n",
    "        # df.drop(columns=[\"message\"], inplace=True)\n",
    "        # clock(\"Extract queries\")\n",
    "\n",
    "        # print(\"Extract parameters: \", end=\"\", flush=True)\n",
    "        # df[\"params\"] = self._extract_params(df[\"detail\"])\n",
    "        # df.drop(columns=[\"detail\"], inplace=True)\n",
    "        # clock(\"Extract parameters\")\n",
    "\n",
    "        # print(\"Substitute parameters into query: \", end=\"\", flush=True)\n",
    "        # df[\"query_subst\"] = self._substitute_params(df, \"query_raw\", \"params\")\n",
    "        # df.drop(columns=[\"query_raw\", \"params\"], inplace=True)\n",
    "        # clock(\"Substitute parameters into query\")\n",
    "        # print(df[\"query_subst\"], \"\\n=====================\")\n",
    "\n",
    "        print(\"Parse query: \", end=\"\", flush=True)\n",
    "        parsed = self._parse(df[\"message\"])\n",
    "        df[[\"query_template\", \"query_params\"]] = pd.DataFrame(\n",
    "            parsed.tolist(), index=df.index)\n",
    "        df.drop(columns=[\"message\"], inplace=True)\n",
    "        clock(\"Parse query\")\n",
    "\n",
    "        # Only keep the relevant columns to optimize for storage, unless otherwise specified.\n",
    "        stored_columns = [\"log_time\", \"query_template\", \"query_params\"]\n",
    "        if store_query_subst:\n",
    "            stored_columns.append(\"query_subst\")\n",
    "        return df[stored_columns]\n",
    "\n",
    "    def __init__(self, workload_csv_path, store_query_subst=False):\n",
    "        log_columns = self._PG_LOG_COLUMNS\n",
    "        print(f\"Preprocessing CSV logs in: {workload_csv_path}\")\n",
    "\n",
    "        df = self._from_csvlogs(\n",
    "            workload_csv_path, log_columns, store_query_subst=store_query_subst)\n",
    "        \n",
    "        df.set_index(\"log_time\", inplace=True)\n",
    "\n",
    "        # Grouping queries by template-parameters count.\n",
    "        gbp = df.groupby([\"query_template\", \"query_params\"]).size()\n",
    "        grouped_by_params = pd.DataFrame(gbp, columns=[\"count\"])\n",
    "        # Remove unrelated queries and empty queries\n",
    "        grouped_by_params = grouped_by_params[~grouped_by_params.index.isin([\n",
    "                                                                            (\"\", ())])]\n",
    "        grouped_by_params = grouped_by_params[~grouped_by_params.index.get_level_values(0).str.contains('pg_', case=False)]\n",
    "        \n",
    "        self._df = df\n",
    "        self._grouped_df_params = grouped_by_params\n",
    "        # self._df = self._df[~(self._df[\"query_template\"].isin([\"\", ()]))]\n",
    "\n",
    "        # Optionally write out the query templates and queries out to a file.\n",
    "        # if self.output_query_templates is not None:\n",
    "        #     templates = preprocessor.get_dataframe()[\"query_template\"]\n",
    "        #     templates = pd.Series(templates[templates != \"\"].unique())\n",
    "        #     templates.to_csv(self.output_query_templates,\n",
    "        #                      header=False, index=False, quoting=csv.QUOTE_ALL)\n",
    "        # if self.output_queries is not None:\n",
    "        #     queries = preprocessor.get_dataframe()[\"query_subst\"]\n",
    "        #     queries = queries[queries != \"\"]\n",
    "        #     queries.to_csv(self.output_queries, header=False,\n",
    "        #                    index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing CSV logs in: ./workload.csv\n",
      "Read dataframe: 0.08 s\n",
      "Parse query: "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/jackiedong/opt/anaconda3/envs/db/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/Users/jackiedong/opt/anaconda3/envs/db/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"/Users/jackiedong/opt/anaconda3/envs/db/lib/python3.7/site-packages/pandarallel/pandarallel.py\", line 64, in global_worker\n    return _func(x)\n  File \"/Users/jackiedong/opt/anaconda3/envs/db/lib/python3.7/site-packages/pandarallel/pandarallel.py\", line 148, in wrapper\n    **kwargs\n  File \"/Users/jackiedong/opt/anaconda3/envs/db/lib/python3.7/site-packages/pandarallel/data_types/series.py\", line 20, in worker\n    return series.apply(func, *args, **kwargs)\n  File \"/Users/jackiedong/opt/anaconda3/envs/db/lib/python3.7/site-packages/pandas/core/series.py\", line 4357, in apply\n    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n  File \"/Users/jackiedong/opt/anaconda3/envs/db/lib/python3.7/site-packages/pandas/core/apply.py\", line 1043, in apply\n    return self.apply_standard()\n  File \"/Users/jackiedong/opt/anaconda3/envs/db/lib/python3.7/site-packages/pandas/core/apply.py\", line 1101, in apply_standard\n    convert=self.convert_dtype,\n  File \"pandas/_libs/lib.pyx\", line 2859, in pandas._libs.lib.map_infer\n  File \"/var/folders/q_/_n20t_vx4tg3802c14dwp1040000gn/T/ipykernel_11596/3057341377.py\", line 322, in parse\n    query = re.search(regex, sql).group(0)\nAttributeError: 'NoneType' object has no attribute 'group'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q_/_n20t_vx4tg3802c14dwp1040000gn/T/ipykernel_11596/4011128672.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./workload.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grouped_df_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/q_/_n20t_vx4tg3802c14dwp1040000gn/T/ipykernel_11596/3057341377.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, workload_csv_path, store_query_subst)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         df = self._from_csvlogs(\n\u001b[0;32m--> 406\u001b[0;31m             workload_csv_path, log_columns, store_query_subst=store_query_subst)\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_time\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/q_/_n20t_vx4tg3802c14dwp1040000gn/T/ipykernel_11596/3057341377.py\u001b[0m in \u001b[0;36m_from_csvlogs\u001b[0;34m(self, workload_csv_path, log_columns, store_query_subst)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Parse query: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         df[[\"query_template\", \"query_params\"]] = pd.DataFrame(\n\u001b[1;32m    391\u001b[0m             parsed.tolist(), index=df.index)\n",
      "\u001b[0;32m/var/folders/q_/_n20t_vx4tg3802c14dwp1040000gn/T/ipykernel_11596/3057341377.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(query_series)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_sql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mquery_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_from_csvlogs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkload_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore_query_subst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/db/lib/python3.7/site-packages/pandarallel/pandarallel.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(data, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0minput_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0moutput_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                 \u001b[0mmap_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m             )\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/db/lib/python3.7/site-packages/pandarallel/pandarallel.py\u001b[0m in \u001b[0;36mget_workers_result\u001b[0;34m(use_memory_fs, nb_workers, show_progress_bar, nb_columns, queue, chunk_lengths, input_files, output_files, map_result)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mprogress_bars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogresses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     return (\n",
      "\u001b[0;32m~/opt/anaconda3/envs/db/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "p = Preprocessor(\"./workload.csv\")\n",
    "p._grouped_df_params[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT * FROM review r, item i WHERE i.i_id = r.i_id and r.i_id=960 ORDER BY rating DESC, r.creation_date DESC LIMIT 10'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most_common_params = (p._grouped_df_params.reset_index().sort_values('count')).drop_duplicates(subset=[\"query_template\"],keep='last')\n",
    "params = p.sample_params(\"SELECT * FROM review r, item i WHERE i.i_id = r.i_id and r.i_id=$1 ORDER BY rating DESC, r.creation_date DESC LIMIT $2\", n=5, replace=True, weights=True)\n",
    "p.substitute_params(\"SELECT * FROM review r, item i WHERE i.i_id = r.i_id and r.i_id=$1 ORDER BY rating DESC, r.creation_date DESC LIMIT $2\", params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query_template\n",
       "SELECT * FROM review r WHERE r.i_id=$1 ORDER BY creation_date DESC                                                            10\n",
       "SELECT * FROM review r, item i WHERE i.i_id = r.i_id and r.i_id=$1 ORDER BY rating DESC, r.creation_date DESC LIMIT $2        10\n",
       "SELECT * FROM review r, useracct u WHERE u.u_id = r.u_id AND r.u_id=$1 ORDER BY rating DESC, r.creation_date DESC LIMIT $2    10\n",
       "SELECT * FROM trust t WHERE t.source_u_id=$1                                                                                  10\n",
       "SELECT avg(rating) FROM review r WHERE r.i_id=$1                                                                              10\n",
       "SELECT avg(rating) FROM review r, trust t WHERE r.u_id=t.target_u_id AND r.i_id=$1 AND t.source_u_id=$2                       10\n",
       "SELECT i_id FROM item                                                                                                          1\n",
       "SELECT u_id FROM useracct                                                                                                      1\n",
       "SELECT version();                                                                                                              1\n",
       "UPDATE item SET title = $1 WHERE i_id=$2                                                                                      10\n",
       "UPDATE review SET rating = $1 WHERE i_id=$2 AND u_id=$3                                                                       10\n",
       "UPDATE trust SET trust = $1 WHERE source_u_id=$2 AND target_u_id=$3                                                           19\n",
       "UPDATE useracct SET name = $1 WHERE u_id=$2                                                                                   10\n",
       "select current_schema()                                                                                                        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count of each query\n",
    "p_count = p._grouped_df_params.groupby(\"query_template\")[\"count\"].sum()\n",
    "scaled_p_count = (p_count * 100/sum(p_count)).apply(np.ceil).astype(int)\n",
    "scaled_p_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 SELECT * FROM review r WHERE r.i_id=$1 ORDER BY creation_date DESC 13734\n"
     ]
    }
   ],
   "source": [
    "for index, (query_template, count) in enumerate(p_count.iteritems()):\n",
    "  print(index, query_template, count)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query_template\n",
       "SELECT * FROM review r WHERE r.i_id=$1 ORDER BY creation_date DESC                                                             1000\n",
       "SELECT * FROM review r, item i WHERE i.i_id = r.i_id and r.i_id=$1 ORDER BY rating DESC, r.creation_date DESC LIMIT $2         1000\n",
       "SELECT * FROM review r, useracct u WHERE u.u_id = r.u_id AND r.u_id=$1 ORDER BY rating DESC, r.creation_date DESC LIMIT $2     2000\n",
       "SELECT * FROM trust t WHERE t.source_u_id=$1                                                                                   2000\n",
       "SELECT avg(rating) FROM review r WHERE r.i_id=$1                                                                               1000\n",
       "SELECT avg(rating) FROM review r, trust t WHERE r.u_id=t.target_u_id AND r.i_id=$1 AND t.source_u_id=$2                       27775\n",
       "SELECT i_id FROM item                                                                                                             1\n",
       "SELECT u_id FROM useracct                                                                                                         1\n",
       "SELECT version();                                                                                                                 1\n",
       "UPDATE item SET title = $1 WHERE i_id=$2                                                                                      27679\n",
       "UPDATE review SET rating = $1 WHERE i_id=$2 AND u_id=$3                                                                       27949\n",
       "UPDATE trust SET trust = $1 WHERE source_u_id=$2 AND target_u_id=$3                                                           55676\n",
       "UPDATE useracct SET name = $1 WHERE u_id=$2                                                                                   27973\n",
       "select current_schema()                                                                                                           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many unique parameters each query has?\n",
    "p_unique = p._grouped_df_params.groupby(\"query_template\").size()\n",
    "p_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(p3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153281"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p._df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for index, row in p._df.iterrows():\n",
    "  print(row[\"query_template\"] == \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
